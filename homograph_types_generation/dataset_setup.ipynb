{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../network_analysis/')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_to_col_name_dict(input_dir):\n",
    "    '''\n",
    "    Given an input directory containing the raw csv files return the `val_to_col_name_dict`\n",
    "    '''\n",
    "    val_to_col_name_dict = {}\n",
    "    # Find the all Homographs and populate the `val_to_col_name_dict`\n",
    "    for filename in tqdm(os.listdir(input_dir)):\n",
    "        df = pd.read_csv(input_dir+filename, keep_default_na=False, dtype=str)\n",
    "        column_names=list(df.columns)\n",
    "        for idx, row in df.iterrows():\n",
    "            for i in range(len(row.tolist())):\n",
    "                if row[i] in val_to_col_name_dict:\n",
    "                    val_to_col_name_dict[row[i]].add(column_names[i])\n",
    "                else:\n",
    "                    val_to_col_name_dict[row[i]] = set([column_names[i]])\n",
    "\n",
    "    return val_to_col_name_dict\n",
    "\n",
    "def get_homographs_and_clean_dataset(input_dir, output_dir, column_name_to_homograph_type, val_to_col_name_dict):\n",
    "    '''\n",
    "    Identifies all homographs and outputs a dataframe with them at `output_dir`.\n",
    "    Also creates a new version on the input tables without he inclusion of the homographs and outputs them at `output_dir/no_homographs/`\n",
    "\n",
    "    Returns the `homograph_info_df` for all types homographs\n",
    "    '''\n",
    "    Path(output_dir+'no_homographs/').mkdir(parents=True, exist_ok=True)\n",
    "    homograph_info_dict={'value': [], 'filename': [], 'column_name' : [], 'type': [], 'subtype': [], \"contents_row\": []}\n",
    "\n",
    "    for filename in tqdm(os.listdir(input_dir)):\n",
    "        df = pd.read_csv(input_dir+filename, keep_default_na=False, dtype=str)\n",
    "        column_names=list(df.columns)\n",
    "\n",
    "        row_ids_with_homographs=set()\n",
    "        for idx, row in df.iterrows():\n",
    "            for i in range(len(row.tolist())):\n",
    "                # Check if value is a homograph\n",
    "                if row[i]=='':\n",
    "                    row_ids_with_homographs.add(idx)\n",
    "                elif len(val_to_col_name_dict[row[i]])>1:\n",
    "                    row_ids_with_homographs.add(idx)\n",
    "                    homograph_info_dict['value'].append(row[i])\n",
    "                    homograph_info_dict['filename'].append(filename)\n",
    "                    homograph_info_dict['column_name'].append(column_names[i])\n",
    "                    homograph_info_dict['type'].append(column_name_to_homograph_type[column_names[i]]['type'])\n",
    "                    homograph_info_dict['subtype'].append(column_name_to_homograph_type[column_names[i]]['subtype'])\n",
    "                    homograph_info_dict['contents_row'].append(row.tolist())\n",
    "            \n",
    "        df_no_homographs = df.drop(labels=row_ids_with_homographs)\n",
    "        df_no_homographs.to_csv(output_dir+'no_homographs/'+filename,index=False)\n",
    "\n",
    "    homograph_info_df = pd.DataFrame.from_dict(homograph_info_dict)\n",
    "    \n",
    "    return homograph_info_df\n",
    "\n",
    "def select_homographs_of_specified_type(homograph_df, num_homographs=10, type='traditional', subtype=None, seed=0):\n",
    "    '''\n",
    "    Returns a list of the selected homographs from the specified types. \n",
    "\n",
    "    Raises an error if it is not possible to extract the specified types\n",
    "    '''\n",
    "\n",
    "    # Check if the requested specification is possible (i.e., there are enough unique homographs)\n",
    "    valid_tuples_df=homograph_df[homograph_df['type']==type]\n",
    "    if subtype: valid_tuples_df=homograph_df[homograph_df['subtype']==subtype]\n",
    "    if valid_tuples_df['value'].nunique() < num_homographs:\n",
    "        raise ValueError('Not possible to extract ' + str(num_homographs) + ' homographs. There are only ' + str(valid_tuples_df['value'].nunique()) + ' available homographs with the specified parameters.')\n",
    "\n",
    "    # Select the homographs\n",
    "    np.random.seed(seed)\n",
    "    selected_homographs=np.random.choice(valid_tuples_df['value'].unique(), size=num_homographs, replace=False)\n",
    "    return selected_homographs\n",
    "\n",
    "def construct_homograph_injected_dataset(input_dir, output_dir, homograph_info_df, num_homographs=10, type='traditional', subtype=None, seed=0, mode=None):\n",
    "    '''\n",
    "    Construct injected homograph datasets of the specified type and amount\n",
    "\n",
    "    `input_dir`: Directory that contains the tables without any homographs\n",
    "    `output_dir`: Directory where the injected homographs dataset is stored\n",
    "    `homograph_info_df`: dataframe that contains the homograph information, types, subtypes, files location etc.\n",
    "    `num_homographs`: number of unique homographs to be inserted overall\n",
    "    `type`: the type of homographs to be inserted\n",
    "    `subtype`: the subtype of homographs to be inserted\n",
    "    `seed`: seed used for the random selection of homographs from the set of valid homographs\n",
    "    `mode`: if specified, then all types are considered, for each type `num_homographs` are selected\n",
    "\n",
    "    Returns a list of the selected homographs\n",
    "    '''\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if mode=='all_types':\n",
    "        # Select homographs from each type\n",
    "        selected_homographs=[]\n",
    "        selected_homographs.extend(select_homographs_of_specified_type(homograph_df=homograph_info_df, num_homographs=num_homographs, type='traditional', subtype=None, seed=seed))\n",
    "        selected_homographs.extend(select_homographs_of_specified_type(homograph_df=homograph_info_df, num_homographs=num_homographs, type='symbolic', subtype='code', seed=seed))\n",
    "        selected_homographs.extend(select_homographs_of_specified_type(homograph_df=homograph_info_df, num_homographs=num_homographs, type='symbolic', subtype='numeric', seed=seed))\n",
    "        selected_homographs.extend(select_homographs_of_specified_type(homograph_df=homograph_info_df, num_homographs=num_homographs, type='null_equivalent', subtype=None, seed=seed))\n",
    "    else:\n",
    "        selected_homographs=select_homographs_of_specified_type(homograph_df=homograph_info_df, num_homographs=num_homographs, type=type, subtype=subtype, seed=seed)\n",
    "\n",
    "    valid_tuples_df=select_valid_tuples(homograph_info_df=homograph_info_df, selected_homographs=selected_homographs, type=type, subtype=subtype, mode=mode)\n",
    "    files_to_modify=set(valid_tuples_df['filename'].unique())\n",
    "    for filename in tqdm(os.listdir(input_dir)):\n",
    "        if filename in files_to_modify:\n",
    "            # Perform insertion\n",
    "            orig_file_df=pd.read_csv(input_dir+filename, keep_default_na=False, dtype=str)\n",
    "            tuples_arr=[]\n",
    "            for idx, row in valid_tuples_df[valid_tuples_df['filename']==filename].iterrows():\n",
    "                tuples_arr.append(row['contents_row'])\n",
    "            new_file_df = pd.concat([orig_file_df, pd.DataFrame(tuples_arr, columns=orig_file_df.columns)])\n",
    "            new_file_df.to_csv(output_dir+filename, index=False)\n",
    "        else:\n",
    "            # Copy the file\n",
    "            shutil.copyfile(input_dir+filename, output_dir+filename)\n",
    "\n",
    "    return list(selected_homographs)\n",
    "\n",
    "def select_valid_tuples(homograph_info_df, selected_homographs, type, subtype, mode):\n",
    "    '''\n",
    "    Returns a subset of the valid tuples from the specified set\n",
    "    '''\n",
    "    if mode=='all_types':\n",
    "        return homograph_info_df[homograph_info_df['value'].isin(selected_homographs)]\n",
    "    elif type=='traditional':\n",
    "        return homograph_info_df[(homograph_info_df['value'].isin(selected_homographs)) & (homograph_info_df['type']==type)]\n",
    "    elif type=='null_equivalent':\n",
    "        return homograph_info_df[(homograph_info_df['value'].isin(selected_homographs)) & (homograph_info_df['type']==type)]\n",
    "    elif type=='symbolic':\n",
    "        return homograph_info_df[(homograph_info_df['value'].isin(selected_homographs)) & (homograph_info_df['subtype']==subtype)]\n",
    "\n",
    "\n",
    "def generate_null_equivalent_tuples(input_dir, homograph_info_df, num_meanings=10, num_values=10, seed=0):\n",
    "    '''\n",
    "    Returns a dataframe with with content tuples to be used for null-equivalent values\n",
    "\n",
    "    `input_dir`: input directory containing the raw csv files to be used for the generation\n",
    "    `homograph_info_df`: dataframe that contains the homograph information, types, subtypes, files location etc., used to ensure no null-equivalent value is found in that dataframe\n",
    "    `num_meanings`: number of meanings of the generated null_equivalent values (i.e., number of unique column_names selected)\n",
    "    `num_values`: number of unique null-equivalent values to be formed\n",
    "    \n",
    "    Returns a dataframe with the tuples for the null equivalent values\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "\n",
    "    # The string values of the null equivalent values\n",
    "    null_equivalent_vals = ['null_equivalent_val_'+str(i) for i in range(num_values)]\n",
    "\n",
    "    # Dictionary mapping each column_name to list of filenames containing them\n",
    "    column_name_to_filenames_dict={}\n",
    "    for filename in os.listdir(input_dir):\n",
    "        columns=pd.read_csv(input_dir+filename, keep_default_na=False, nrows=0, dtype=str).columns.to_list()\n",
    "        for col in columns:\n",
    "            if col not in column_name_to_filenames_dict:\n",
    "                column_name_to_filenames_dict[col]=[filename]\n",
    "            else:\n",
    "                column_name_to_filenames_dict[col].append(filename)\n",
    "    \n",
    "        # Select `num_meanings` (filename, column_name) pairs for each injected_null_equivalent_val\n",
    "    null_val_to_filename_column_name_pairs={}\n",
    "    for val in null_equivalent_vals:\n",
    "        selected_column_names=random.choices(list(column_name_to_filenames_dict.keys()), k=num_meanings)\n",
    "        filename_column_name_tuples=[(random.choice(column_name_to_filenames_dict[column_name]), column_name) for column_name in selected_column_names]\n",
    "        null_val_to_filename_column_name_pairs[val]=filename_column_name_tuples\n",
    "    \n",
    "    # Construct the tuples and populate the tuples_dict\n",
    "    tuples_dict={'value': [], 'filename': [], 'column_name' : [], 'type': [], 'subtype': [], \"contents_row\": []}\n",
    "    for val in tqdm(null_equivalent_vals):\n",
    "        for pair in null_val_to_filename_column_name_pairs[val]:\n",
    "            filename, column_name=pair[0], pair[1]\n",
    "            df=pd.read_csv(input_dir+filename, keep_default_na=False, dtype=str)\n",
    "\n",
    "            # Sample a row and modify the value from the selected column_name\n",
    "            row = df.sample(n=1, random_state=seed)\n",
    "            row[column_name]=val\n",
    "\n",
    "            # Update the tuples_dict\n",
    "            tuples_dict['value'].append(val)\n",
    "            tuples_dict['filename'].append(filename)\n",
    "            tuples_dict['column_name'].append(column_name)\n",
    "            tuples_dict['type'].append('null_equivalent')\n",
    "            tuples_dict['subtype'].append(None)\n",
    "            tuples_dict['contents_row'].append(row.values[0])\n",
    "    \n",
    "    return pd.DataFrame.from_dict(tuples_dict)\n",
    "\n",
    "def get_df_subset_from_mode(df, mode='homographs_traditional'):\n",
    "    if mode=='homographs_traditional':\n",
    "        return df[df['type']=='traditional']\n",
    "    elif mode=='homographs_symbolic_code':\n",
    "        return df[df['subtype']=='code']\n",
    "    elif mode=='homographs_symbolic_numeric':\n",
    "        return df[df['subtype']=='numeric']\n",
    "\n",
    "def populate_homograph_info_dict(homograph_info_dict):\n",
    "    all_types_df=homograph_info_dict['homographs_all']\n",
    "    modes=['homographs_traditional', 'homographs_symbolic_code', 'homographs_symbolic_numeric']\n",
    "\n",
    "    for mode in tqdm(modes):\n",
    "        cur_df=get_df_subset_from_mode(df=all_types_df, mode=mode)\n",
    "        selected_vals=[]\n",
    "        for val in cur_df['value'].unique():\n",
    "            if cur_df[cur_df['value']==val]['column_name'].nunique()>1:\n",
    "                selected_vals.append(val)\n",
    "        homograph_info_dict[mode]=cur_df[cur_df['value'].isin(selected_vals)]      \n",
    "\n",
    "    return homograph_info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homograph Injection and Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:11<00:00, 16.80it/s]\n",
      "100%|██████████| 192/192 [00:11<00:00, 16.15it/s]\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "raw_input_dir='../DATA/synthetic_benchmark_large3/'\n",
    "output_dir='datasets/synthetic_benchmark_large3/'\n",
    "\n",
    "with open('column_name_to_homograph_type.json') as f:\n",
    "    column_name_to_homograph_type = json.load(f)\n",
    "\n",
    "# Dictionary mapping each value to a list of column names\n",
    "val_to_col_name_dict = get_val_to_col_name_dict(input_dir=raw_input_dir)\n",
    "\n",
    "# Construct the homograph_info_dict and populate with all types of homographs\n",
    "homograph_info_dict={}\n",
    "homograph_info_dict['homographs_all']=get_homographs_and_clean_dataset(output_dir=output_dir, input_dir=raw_input_dir, column_name_to_homograph_type=column_name_to_homograph_type, val_to_col_name_dict=val_to_col_name_dict)\n",
    "homograph_info_dict = populate_homograph_info_dict(homograph_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input dataset contains: 2740 unique all homograph values.\n",
      "There are: 526 unique traditional only homographs 1083 unique symbolic(code) only homographs 906 unique symbolic(numeric) only homographs\n"
     ]
    }
   ],
   "source": [
    "print(\"The input dataset contains:\", homograph_info_dict['homographs_all']['value'].nunique(), \"unique all homograph values.\")\n",
    "print(\"There are:\", homograph_info_dict['homographs_traditional']['value'].nunique(), 'unique traditional only homographs', \n",
    "homograph_info_dict['homographs_symbolic_code']['value'].nunique(), 'unique symbolic(code) only homographs',\n",
    "homograph_info_dict['homographs_symbolic_numeric']['value'].nunique(), 'unique symbolic(numeric) only homographs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 51.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Append the null_tuples_df into the homograph_info_df\n",
    "null_tuples_df = generate_null_equivalent_tuples(input_dir=raw_input_dir, homograph_info_df=homograph_info_dict['homographs_all'], num_meanings=10, num_values=100)\n",
    "homograph_info_dict['homographs_all']=pd.concat([homograph_info_dict['homographs_all'], null_tuples_df])\n",
    "with open('datasets/synthetic_benchmark_large3/homograph_info_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(homograph_info_dict, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injected Datasets Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:00<00:00, 217.22it/s]\n",
      "100%|██████████| 192/192 [00:00<00:00, 748.93it/s]\n",
      "100%|██████████| 192/192 [00:00<00:00, 509.46it/s]\n",
      "100%|██████████| 192/192 [00:00<00:00, 247.61it/s]\n",
      "100%|██████████| 192/192 [00:01<00:00, 131.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# homograph_info_df=pd.read_pickle(output_dir+'homograph_info_df.pickle')\n",
    "with open('datasets/synthetic_benchmark_large3/homograph_info_dict.pickle', 'rb') as handle:\n",
    "    homograph_info_dict = pickle.load(handle)\n",
    "num_homographs=100\n",
    "modes=['homographs_traditional', 'homographs_symbolic_code', 'homographs_symbolic_numeric']\n",
    "homs_dict = {}\n",
    "for mode in modes:\n",
    "    subtype=None\n",
    "    if mode=='homographs_traditional':\n",
    "        type='traditional'\n",
    "    elif mode=='homographs_symbolic_code':\n",
    "        type='symbolic';subtype='code'\n",
    "    elif mode=='homographs_symbolic_numeric':\n",
    "        type='symbolic';subtype='numeric'\n",
    "    homs = construct_homograph_injected_dataset(input_dir=output_dir+'no_homographs/', output_dir=output_dir+mode+'_'+str(num_homographs)+'/', homograph_info_df=homograph_info_dict[mode], num_homographs=num_homographs, type=type, subtype=subtype, seed=0)\n",
    "    homs_dict[mode]=homs\n",
    "    \n",
    "homs_dict['homographs_null_equivalent'] = construct_homograph_injected_dataset(input_dir=output_dir+'no_homographs/', output_dir=output_dir+'homographs_null_equivalent_'+str(num_homographs)+'/', homograph_info_df=homograph_info_dict['homographs_all'], num_homographs=num_homographs, type='null_equivalent', seed=0)\n",
    "\n",
    "# TODO: Ensure correct insertion for all types homographs (ensuring exclusive per type insertion)\n",
    "homs_dict['homographs_all'] = construct_homograph_injected_dataset(input_dir=output_dir+'no_homographs/', output_dir=output_dir+'homographs_all_'+str(num_homographs)+'/', homograph_info_df=homograph_info_dict['homographs_all'], num_homographs=num_homographs, type=None, seed=0, mode='all_types')\n",
    "\n",
    "with open(output_dir+'selected_homographs.json', 'w') as f:\n",
    "    json.dump(homs_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119518 187745\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../graph_construction/combined_graphs_output/synthetic_benchmark_large3/homographs_symbolic_numeric_100/bipartite/bipartite.graph', 'rb') as f:\n",
    "    G=pickle.load(f)\n",
    "print(G.number_of_nodes(), G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airport_elevation_airport_elevation_1.csv',\n",
       " 'airport_elevation_airport_elevation_2.csv',\n",
       " 'airport_elevation_airport_elevation_3.csv',\n",
       " 'airport_elevation_airport_elevation_4.csv',\n",
       " 'airport_elevation_airport_elevation_8.csv',\n",
       " 'airport_elevation_airport_elevation_9.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_1.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_10.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_11.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_12.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_2.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_3.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_4.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_5.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_6.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_7.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_8.csv',\n",
       " 'car_model_year_car_make_car_model_car_model_year_9.csv']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_attribute_of_instance(G, '2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>filename</th>\n",
       "      <th>column_name</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>contents_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>airport_elevation_9.csv</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231</td>\n",
       "      <td>airport_elevation_9.csv</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[231]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212</td>\n",
       "      <td>airport_elevation_9.csv</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[212]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>218</td>\n",
       "      <td>airport_elevation_9.csv</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[218]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108</td>\n",
       "      <td>airport_elevation_9.csv</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[108]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72748</th>\n",
       "      <td>995</td>\n",
       "      <td>row_id_12.csv</td>\n",
       "      <td>row_id</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[995]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72749</th>\n",
       "      <td>996</td>\n",
       "      <td>row_id_12.csv</td>\n",
       "      <td>row_id</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72750</th>\n",
       "      <td>997</td>\n",
       "      <td>row_id_12.csv</td>\n",
       "      <td>row_id</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72751</th>\n",
       "      <td>999</td>\n",
       "      <td>row_id_12.csv</td>\n",
       "      <td>row_id</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[999]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72752</th>\n",
       "      <td>1000</td>\n",
       "      <td>row_id_12.csv</td>\n",
       "      <td>row_id</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[1000]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34335 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value                 filename        column_name      type  subtype  \\\n",
       "0        32  airport_elevation_9.csv  airport_elevation  symbolic  numeric   \n",
       "1       231  airport_elevation_9.csv  airport_elevation  symbolic  numeric   \n",
       "2       212  airport_elevation_9.csv  airport_elevation  symbolic  numeric   \n",
       "3       218  airport_elevation_9.csv  airport_elevation  symbolic  numeric   \n",
       "4       108  airport_elevation_9.csv  airport_elevation  symbolic  numeric   \n",
       "...     ...                      ...                ...       ...      ...   \n",
       "72748   995            row_id_12.csv             row_id  symbolic  numeric   \n",
       "72749   996            row_id_12.csv             row_id  symbolic  numeric   \n",
       "72750   997            row_id_12.csv             row_id  symbolic  numeric   \n",
       "72751   999            row_id_12.csv             row_id  symbolic  numeric   \n",
       "72752  1000            row_id_12.csv             row_id  symbolic  numeric   \n",
       "\n",
       "      contents_row  \n",
       "0             [32]  \n",
       "1            [231]  \n",
       "2            [212]  \n",
       "3            [218]  \n",
       "4            [108]  \n",
       "...            ...  \n",
       "72748        [995]  \n",
       "72749        [996]  \n",
       "72750        [997]  \n",
       "72751        [999]  \n",
       "72752       [1000]  \n",
       "\n",
       "[34335 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homograph_info_dict['homographs_symbolic_numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>node_type</th>\n",
       "      <th>approximate_betweenness_centrality</th>\n",
       "      <th>is_homograph</th>\n",
       "      <th>homograph_mode</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>8508</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5504</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>26009 CEDEX</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>8118</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Philipet</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Clemenson</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Lehr</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Lafaye</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Baber</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Fonzo</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>McGlynn</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Rambaut</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Scarrisbrick</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Woof</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Chaston</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Longridge</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>O'Quin</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Kunzel</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Bevington</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>McGuinley</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Mewburn</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Foxwell</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>9803</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>95000-000</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>4201</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>50601</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>McKaile</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Currm</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Rainsdon</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Sollom</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>McKew</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Tesche</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Menilove</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Youell</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Duffie</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Quare</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Earngy</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Tabard</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Revans</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Maharg</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Eyer</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Dyhouse</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Chidzoy</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Krates</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Pywell</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Paraman</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Cahillane</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Clow</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Cremins</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Vouls</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Beadon</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Stonestreet</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Minelli</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Jagg</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Gilluley</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Pitford</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>8305</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>IT University of Copenhagen</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>East Delta University</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Staniforth</td>\n",
       "      <td>cell</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            node node_type  \\\n",
       "100                         8508      cell   \n",
       "101                         5504      cell   \n",
       "102                  26009 CEDEX      cell   \n",
       "103                         8118      cell   \n",
       "104                     Philipet      cell   \n",
       "105                    Clemenson      cell   \n",
       "106                         Lehr      cell   \n",
       "107                       Lafaye      cell   \n",
       "108                        Baber      cell   \n",
       "109                        Fonzo      cell   \n",
       "110                      McGlynn      cell   \n",
       "111                      Rambaut      cell   \n",
       "112                 Scarrisbrick      cell   \n",
       "113                         Woof      cell   \n",
       "114                      Chaston      cell   \n",
       "115                    Longridge      cell   \n",
       "116                       O'Quin      cell   \n",
       "117                       Kunzel      cell   \n",
       "118                    Bevington      cell   \n",
       "119                    McGuinley      cell   \n",
       "120                      Mewburn      cell   \n",
       "121                      Foxwell      cell   \n",
       "122                         9803      cell   \n",
       "123                    95000-000      cell   \n",
       "124                         4201      cell   \n",
       "125                        50601      cell   \n",
       "126                      McKaile      cell   \n",
       "127                        Currm      cell   \n",
       "128                     Rainsdon      cell   \n",
       "129                       Sollom      cell   \n",
       "130                        McKew      cell   \n",
       "131                       Tesche      cell   \n",
       "132                     Menilove      cell   \n",
       "133                       Youell      cell   \n",
       "134                       Duffie      cell   \n",
       "135                        Quare      cell   \n",
       "136                       Earngy      cell   \n",
       "137                       Tabard      cell   \n",
       "138                       Revans      cell   \n",
       "139                       Maharg      cell   \n",
       "140                         Eyer      cell   \n",
       "141                      Dyhouse      cell   \n",
       "142                      Chidzoy      cell   \n",
       "143                       Krates      cell   \n",
       "144                       Pywell      cell   \n",
       "145                      Paraman      cell   \n",
       "146                    Cahillane      cell   \n",
       "147                         Clow      cell   \n",
       "148                      Cremins      cell   \n",
       "149                        Vouls      cell   \n",
       "150                       Beadon      cell   \n",
       "151                  Stonestreet      cell   \n",
       "152                      Minelli      cell   \n",
       "153                         Jagg      cell   \n",
       "154                     Gilluley      cell   \n",
       "155                      Pitford      cell   \n",
       "156                         8305      cell   \n",
       "157  IT University of Copenhagen      cell   \n",
       "158        East Delta University      cell   \n",
       "159                   Staniforth      cell   \n",
       "\n",
       "     approximate_betweenness_centrality  is_homograph homograph_mode  \\\n",
       "100                            0.000868         False            NaN   \n",
       "101                            0.000858         False            NaN   \n",
       "102                            0.000846         False            NaN   \n",
       "103                            0.000838         False            NaN   \n",
       "104                            0.000828         False            NaN   \n",
       "105                            0.000828         False            NaN   \n",
       "106                            0.000828         False            NaN   \n",
       "107                            0.000828         False            NaN   \n",
       "108                            0.000828         False            NaN   \n",
       "109                            0.000828         False            NaN   \n",
       "110                            0.000824         False            NaN   \n",
       "111                            0.000824         False            NaN   \n",
       "112                            0.000824         False            NaN   \n",
       "113                            0.000824         False            NaN   \n",
       "114                            0.000824         False            NaN   \n",
       "115                            0.000824         False            NaN   \n",
       "116                            0.000819         False            NaN   \n",
       "117                            0.000819         False            NaN   \n",
       "118                            0.000819         False            NaN   \n",
       "119                            0.000819         False            NaN   \n",
       "120                            0.000819         False            NaN   \n",
       "121                            0.000819         False            NaN   \n",
       "122                            0.000813         False            NaN   \n",
       "123                            0.000810         False            NaN   \n",
       "124                            0.000807         False            NaN   \n",
       "125                            0.000804         False            NaN   \n",
       "126                            0.000792         False            NaN   \n",
       "127                            0.000792         False            NaN   \n",
       "128                            0.000792         False            NaN   \n",
       "129                            0.000792         False            NaN   \n",
       "130                            0.000792         False            NaN   \n",
       "131                            0.000792         False            NaN   \n",
       "132                            0.000785         False            NaN   \n",
       "133                            0.000785         False            NaN   \n",
       "134                            0.000785         False            NaN   \n",
       "135                            0.000785         False            NaN   \n",
       "136                            0.000785         False            NaN   \n",
       "137                            0.000779         False            NaN   \n",
       "138                            0.000779         False            NaN   \n",
       "139                            0.000779         False            NaN   \n",
       "140                            0.000779         False            NaN   \n",
       "141                            0.000779         False            NaN   \n",
       "142                            0.000775         False            NaN   \n",
       "143                            0.000775         False            NaN   \n",
       "144                            0.000775         False            NaN   \n",
       "145                            0.000775         False            NaN   \n",
       "146                            0.000775         False            NaN   \n",
       "147                            0.000775         False            NaN   \n",
       "148                            0.000775         False            NaN   \n",
       "149                            0.000770         False            NaN   \n",
       "150                            0.000770         False            NaN   \n",
       "151                            0.000770         False            NaN   \n",
       "152                            0.000770         False            NaN   \n",
       "153                            0.000770         False            NaN   \n",
       "154                            0.000770         False            NaN   \n",
       "155                            0.000770         False            NaN   \n",
       "156                            0.000770         False            NaN   \n",
       "157                            0.000766         False            NaN   \n",
       "158                            0.000763         False            NaN   \n",
       "159                            0.000763         False            NaN   \n",
       "\n",
       "     precision  recall  f1_score  \n",
       "100        0.0     0.0       0.0  \n",
       "101        0.0     0.0       0.0  \n",
       "102        0.0     0.0       0.0  \n",
       "103        0.0     0.0       0.0  \n",
       "104        0.0     0.0       0.0  \n",
       "105        0.0     0.0       0.0  \n",
       "106        0.0     0.0       0.0  \n",
       "107        0.0     0.0       0.0  \n",
       "108        0.0     0.0       0.0  \n",
       "109        0.0     0.0       0.0  \n",
       "110        0.0     0.0       0.0  \n",
       "111        0.0     0.0       0.0  \n",
       "112        0.0     0.0       0.0  \n",
       "113        0.0     0.0       0.0  \n",
       "114        0.0     0.0       0.0  \n",
       "115        0.0     0.0       0.0  \n",
       "116        0.0     0.0       0.0  \n",
       "117        0.0     0.0       0.0  \n",
       "118        0.0     0.0       0.0  \n",
       "119        0.0     0.0       0.0  \n",
       "120        0.0     0.0       0.0  \n",
       "121        0.0     0.0       0.0  \n",
       "122        0.0     0.0       0.0  \n",
       "123        0.0     0.0       0.0  \n",
       "124        0.0     0.0       0.0  \n",
       "125        0.0     0.0       0.0  \n",
       "126        0.0     0.0       0.0  \n",
       "127        0.0     0.0       0.0  \n",
       "128        0.0     0.0       0.0  \n",
       "129        0.0     0.0       0.0  \n",
       "130        0.0     0.0       0.0  \n",
       "131        0.0     0.0       0.0  \n",
       "132        0.0     0.0       0.0  \n",
       "133        0.0     0.0       0.0  \n",
       "134        0.0     0.0       0.0  \n",
       "135        0.0     0.0       0.0  \n",
       "136        0.0     0.0       0.0  \n",
       "137        0.0     0.0       0.0  \n",
       "138        0.0     0.0       0.0  \n",
       "139        0.0     0.0       0.0  \n",
       "140        0.0     0.0       0.0  \n",
       "141        0.0     0.0       0.0  \n",
       "142        0.0     0.0       0.0  \n",
       "143        0.0     0.0       0.0  \n",
       "144        0.0     0.0       0.0  \n",
       "145        0.0     0.0       0.0  \n",
       "146        0.0     0.0       0.0  \n",
       "147        0.0     0.0       0.0  \n",
       "148        0.0     0.0       0.0  \n",
       "149        0.0     0.0       0.0  \n",
       "150        0.0     0.0       0.0  \n",
       "151        0.0     0.0       0.0  \n",
       "152        0.0     0.0       0.0  \n",
       "153        0.0     0.0       0.0  \n",
       "154        0.0     0.0       0.0  \n",
       "155        0.0     0.0       0.0  \n",
       "156        0.0     0.0       0.0  \n",
       "157        0.0     0.0       0.0  \n",
       "158        0.0     0.0       0.0  \n",
       "159        0.0     0.0       0.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('evaluation/synthetic_benchmark_large3/eval_dfs.pickle')['homographs_symbolic_numeric']\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.iloc[100:160]\n",
    "# df[df['is_homograph']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Colorado', 'Duff', 'Elan', 'Garvey', 'Elmira', 'Berkeley', 'Ram',\n",
       "       'Charity', 'California', 'Crossfire'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "np.random.choice(homograph_info_df[(homograph_info_df['type']=='traditional')]['value'].unique(), size=10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ID', 'NE', 'GT', 'AR', 'CO', 'MA', 'CA', 'DE', 'TL', 'MN', 'AL',\n",
       "       'SD', 'PA', 'AZ', 'TN', 'CT', 'SC', 'IL', 'GA', 'MD', 'ME',\n",
       "       'Colorado', 'Elan', 'ES', 'Crossfire', 'Ram', 'California',\n",
       "       'Jimmy', nan], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homograph_info_df[homograph_info_df['type']=='symbolic']['value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airport_elevation', 'cost', 'row_id'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_to_col_name_dict[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 12.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>filename</th>\n",
       "      <th>column_name</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>contents_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_code</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>code</td>\n",
       "      <td>[0, Usina Mandu Airport, 1597]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[CCK, Cocos (Keeling) Islands Airport, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_code</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>code</td>\n",
       "      <td>[0, Venâncio Aires Airport, 226]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[NDA, Bandanaira Airport, 100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[ARD, Mali Airport, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>900</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[ZZV, Zanesville Municipal Airport, 900]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>13</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[ARO, Arboletes Airport, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>626</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[LNN, Willoughby Lost Nation Municipal Airport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>9</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[FID, Elizabeth Field, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>100</td>\n",
       "      <td>row_id_airport_code_continent_code_airport_ele...</td>\n",
       "      <td>airport_elevation</td>\n",
       "      <td>symbolic</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[MDM, Munduku Airport, 100]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1144 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     value                                           filename  \\\n",
       "0        0  row_id_airport_code_continent_code_airport_ele...   \n",
       "1       10  row_id_airport_code_continent_code_airport_ele...   \n",
       "2        0  row_id_airport_code_continent_code_airport_ele...   \n",
       "3      100  row_id_airport_code_continent_code_airport_ele...   \n",
       "4       10  row_id_airport_code_continent_code_airport_ele...   \n",
       "...    ...                                                ...   \n",
       "1139   900  row_id_airport_code_continent_code_airport_ele...   \n",
       "1140    13  row_id_airport_code_continent_code_airport_ele...   \n",
       "1141   626  row_id_airport_code_continent_code_airport_ele...   \n",
       "1142     9  row_id_airport_code_continent_code_airport_ele...   \n",
       "1143   100  row_id_airport_code_continent_code_airport_ele...   \n",
       "\n",
       "            column_name      type  subtype  \\\n",
       "0          airport_code  symbolic     code   \n",
       "1     airport_elevation  symbolic  numeric   \n",
       "2          airport_code  symbolic     code   \n",
       "3     airport_elevation  symbolic  numeric   \n",
       "4     airport_elevation  symbolic  numeric   \n",
       "...                 ...       ...      ...   \n",
       "1139  airport_elevation  symbolic  numeric   \n",
       "1140  airport_elevation  symbolic  numeric   \n",
       "1141  airport_elevation  symbolic  numeric   \n",
       "1142  airport_elevation  symbolic  numeric   \n",
       "1143  airport_elevation  symbolic  numeric   \n",
       "\n",
       "                                           contents_row  \n",
       "0                        [0, Usina Mandu Airport, 1597]  \n",
       "1            [CCK, Cocos (Keeling) Islands Airport, 10]  \n",
       "2                      [0, Venâncio Aires Airport, 226]  \n",
       "3                        [NDA, Bandanaira Airport, 100]  \n",
       "4                               [ARD, Mali Airport, 10]  \n",
       "...                                                 ...  \n",
       "1139           [ZZV, Zanesville Municipal Airport, 900]  \n",
       "1140                       [ARO, Arboletes Airport, 13]  \n",
       "1141  [LNN, Willoughby Lost Nation Municipal Airport...  \n",
       "1142                          [FID, Elizabeth Field, 9]  \n",
       "1143                        [MDM, Munduku Airport, 100]  \n",
       "\n",
       "[1144 rows x 6 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homs_tmp=get_homographs_and_clean_dataset(output_dir='../DATA/tmp/', input_dir='../DATA/testing/', column_name_to_homograph_type=column_name_to_homograph_type, val_to_col_name_dict=val_to_col_name_dict)\n",
    "homs_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '10', '100', '9', '13', '200', '90', '15', '650', '98', '164',\n",
       "       '545', '42', '760', 'RVR', '325', '740', 'DNA', '88', '940', 'SIS',\n",
       "       '3500', 'MKZ', '80', '300', '1500', 'TSX', '207', '1', '6000',\n",
       "       '928', 'MPV', '120', '228', 'LHS', '57', '600', '745', 'SRX',\n",
       "       '626', 'LUV', 'GTO', '430', '323', 'SLX', '2500', '750', '924',\n",
       "       'GLI', '550', '62', '911', 'SSR', 'LOL', '960', '900', '9000',\n",
       "       '525', '240', '500', '944', '330', 'MKS', '645', 'STS', '1000',\n",
       "       'GTI', '968', 'APV', '914', '929', 'DBS', 'GLC', 'LSS', 'RSX'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homs_tmp['value'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
