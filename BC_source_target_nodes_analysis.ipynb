{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import statistics\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(dir_path, source_target):\n",
    "    \n",
    "    measures_dict = {\n",
    "        'source_target': [],\n",
    "        'at_cutoff_rank': [], 'at_cutoff_precision': [], 'at_cutoff_recall': [], 'at_cutoff_f1_score': [],\n",
    "        'at_max_f1_score_rank': [], 'at_max_f1_score_precision': [], 'at_max_f1_score_recall': [], 'at_max_f1_score_f1_score': []\n",
    "    }\n",
    "\n",
    "    for dir in os.listdir(dir_path):\n",
    "        with open(dir_path+dir+'/results_summary.json') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            measures_dict['source_target'].append(source_target)\n",
    "\n",
    "            # measures at cut off\n",
    "            measures_at_cut_off_dict = data['measures_at_cut_off']\n",
    "            measures_dict['at_cutoff_rank'].append(measures_at_cut_off_dict['rank'])\n",
    "            measures_dict['at_cutoff_precision'].append(measures_at_cut_off_dict['precision'])\n",
    "            measures_dict['at_cutoff_recall'].append(measures_at_cut_off_dict['recall'])\n",
    "            measures_dict['at_cutoff_f1_score'].append(measures_at_cut_off_dict['f1_score'])\n",
    "\n",
    "            # measures at max f1-score\n",
    "            measures_at_max_f1_score = data['measures_at_max_f1_score']\n",
    "            measures_dict['at_max_f1_score_rank'].append(measures_at_max_f1_score['rank'])\n",
    "            measures_dict['at_max_f1_score_precision'].append(measures_at_max_f1_score['precision'])\n",
    "            measures_dict['at_max_f1_score_recall'].append(measures_at_max_f1_score['recall'])\n",
    "            measures_dict['at_max_f1_score_f1_score'].append(measures_at_max_f1_score['f1_score'])\n",
    "        \n",
    "    return measures_dict"
   ]
  },
  {
   "source": [
    "# Approximate BC by varying the source and target nodes used"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_main_path = 'network_analysis/figures/'\n",
    "name_types = ['all', 'cell', 'attr']\n",
    "df = pd.DataFrame()\n",
    "dir_list = []\n",
    "\n",
    "for source in name_types:\n",
    "    for target in name_types:\n",
    "        source_target = source + '_' + target\n",
    "        cur_dir = dir_main_path+'TUS_source_' + source + '_target_' + target + '/'\n",
    "\n",
    "        if os.path.isdir(cur_dir):\n",
    "            # Retreive the stat dict and populate the dataframe\n",
    "            stat_dict = get_statistics(cur_dir, source_target=source_target)\n",
    "            df_temp = pd.DataFrame(stat_dict)\n",
    "            df = df.append(df_temp, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean values in each measure\n",
    "df_summary_mean = df.groupby(['source_target'], as_index=False).mean()\n",
    "df_summary_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation of values in each measure\n",
    "df_summary_std = df.groupby(['source_target'], as_index=False).std()\n",
    "\n",
    "rename_dict = {}\n",
    "for column in df_summary_std.columns:\n",
    "    if column not in ['source_target']:\n",
    "        rename_dict[column] = column+'_std'\n",
    "\n",
    "df_summary_std.rename(columns=rename_dict, inplace=True)\n",
    "df_summary_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes and generate a summarized dataframe for relevant columns\n",
    "columns_to_select = [\n",
    "    'source_target', 'at_cutoff_f1_score', 'at_cutoff_f1_score_std', 'at_max_f1_score_rank', 'at_max_f1_score_rank_std',\n",
    "    'at_max_f1_score_precision', 'at_max_f1_score_precision_std', 'at_max_f1_score_recall', 'at_max_f1_score_recall_std', \n",
    "    'at_max_f1_score_f1_score', 'at_max_f1_score_f1_score_std'\n",
    "]\n",
    "\n",
    "df_summary = pd.merge(df_summary_mean, df_summary_std, on=\"source_target\")[columns_to_select]\n",
    "df_summary"
   ]
  },
  {
   "source": [
    "# Graph Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (19,9)\n",
    "\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_path = 'graph_construction/combined_graphs_output/TUS/bipartite/bipartite.graph'\n",
    "df_path = 'network_analysis/output/TUS_source_all_target_all/seed1/graph_stats_with_groundtruth_df.pickle'\n",
    "G = pickle.load(open(g_path, 'rb'))\n",
    "df = pickle.load(open(df_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_nodes = [x for x,y in G.nodes(data=True) if y['type']=='cell']\n",
    "attr_nodes = [x for x,y in G.nodes(data=True) if y['type']=='attr']\n",
    "homograph_nodes = df.loc[df['is_homograph'] == True]['node'].values\n",
    "identical_nodes = df.loc[df['is_homograph'] == False]['node'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_neighbors_list(G, nodes):\n",
    "    '''\n",
    "    Given a list of nodes from graph `G` return the number of neighbors for each node in the nodes list.\n",
    "\n",
    "    The returned list size has the same size as the `nodes` list\n",
    "    '''\n",
    "    num_neighbors = []\n",
    "    for node in nodes:\n",
    "        num_neighbors.append(len(G[node]))    \n",
    "    return num_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_nodes_num_neighbors = num_neighbors_list(G, cell_nodes)\n",
    "attr_nodes_num_neighbors = num_neighbors_list(G, attr_nodes)\n",
    "homograph_nodes_num_neighbors = num_neighbors_list(G, homograph_nodes)\n",
    "identical_nodes_num_neighbors = num_neighbors_list(G, identical_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cell Nodes mean num neighbors:', statistics.mean(cell_nodes_num_neighbors), 'median:', statistics.median(cell_nodes_num_neighbors))\n",
    "print('Attribute Nodes mean num neighbors:', statistics.mean(attr_nodes_num_neighbors), 'median:', statistics.median(attr_nodes_num_neighbors))\n",
    "print('Homograph Nodes mean num neighbors:', statistics.mean(homograph_nodes_num_neighbors), 'median:', statistics.median(homograph_nodes_num_neighbors))\n",
    "print('Unambiguous Nodes Nodes mean num neighbors:', statistics.mean(identical_nodes_num_neighbors), 'median:', statistics.median(identical_nodes_num_neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cell_nodes_num_neighbors, bins=1000)\n",
    "plt.xlim([0, 550])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('network_analysis/figures/BC_source_target_nodes_analysis/cell_nodes_num_neighbors.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(attr_nodes_num_neighbors, bins=400)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('network_analysis/figures/BC_source_target_nodes_analysis/attr_nodes_num_neighbors.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([homograph_nodes_num_neighbors, identical_nodes_num_neighbors], bins=400, label=['homographs', 'unambiguous values'])\n",
    "plt.xlim([0, 550])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('network_analysis/figures/BC_source_target_nodes_analysis/homographs_identical_vals_num_neighbors.svg')"
   ]
  }
 ]
}