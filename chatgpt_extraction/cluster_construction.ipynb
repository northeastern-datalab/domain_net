{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_assignment(files_path):\n",
    "    '''\n",
    "    Returns a dictionary assigning each column_filename of a homograph to a cluster id\n",
    "\n",
    "    Algorithm: Initialy each file is in a separate cluster ID. Clusters IDs merge if the pair is considered not to be homoraphs\n",
    "    '''\n",
    "    file_to_cluster_id={}\n",
    "    cur_id=0\n",
    "    for pair in os.listdir(files_path):        \n",
    "        file1=pair.split('___')[0] + '.csv'\n",
    "        file2=pair.split('___')[1].split('.')[0] + '.csv'\n",
    "\n",
    "        if file1 not in file_to_cluster_id:\n",
    "            file_to_cluster_id[file1]=cur_id\n",
    "            cur_id+=1\n",
    "        if file2 not in file_to_cluster_id:\n",
    "            file_to_cluster_id[file2]=cur_id\n",
    "            cur_id+=1\n",
    "    \n",
    "    for pair in os.listdir(files_path):\n",
    "        with open(files_path+'/'+pair, 'r') as f:\n",
    "            is_homograph=json.load(f)['is_homograph']\n",
    "        file1=pair.split('___')[0] + '.csv'\n",
    "        file2=pair.split('___')[1].split('.')[0] + '.csv'\n",
    "\n",
    "        if is_homograph=='False':\n",
    "            val_to_set=min(file_to_cluster_id[file1], file_to_cluster_id[file2])\n",
    "            file_to_cluster_id[file1]=val_to_set\n",
    "            file_to_cluster_id[file2]=val_to_set\n",
    "    \n",
    "    return file_to_cluster_id\n",
    "\n",
    "def get_clustering_evaluation_score(labels_pred, idx_to_nodes, G, measure='adj_rand_index'):\n",
    "    '''\n",
    "    Given `labels_pred` which is an assignment of types for the attributes of a node, derive the groundtruth\n",
    "    using `idx_to_nodes` and `G` and return the specified clustering evaluation `measure` \n",
    "\n",
    "    Allowed measures: ['adj_rand_index', 'adj_mutual_info', 'norm_mutual_info']\n",
    "    '''\n",
    "    # Derive the groundtruth labels by getting the column name for each attribute in the idx_to_nodes list\n",
    "    col_names = [G.nodes[attr]['column_name'] for attr in idx_to_nodes.values()]\n",
    "    col_name_to_label_dict = dict([(y,x) for x,y in enumerate(sorted(set(col_names)))])\n",
    "    labels_true = [col_name_to_label_dict[x] for x in col_names]\n",
    "    \n",
    "    if measure == 'adj_rand_index':\n",
    "        score = metrics.adjusted_rand_score(labels_true=labels_true, labels_pred=labels_pred)\n",
    "    elif measure == 'adj_mutual_info':\n",
    "        score = metrics.adjusted_mutual_info_score(labels_true=labels_true, labels_pred=labels_pred)\n",
    "    elif measure == 'norm_mutual_info':\n",
    "        score = metrics.normalized_mutual_info_score(labels_true=labels_true, labels_pred=labels_pred)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir='gpt_output/synthetic_benchmark_large/'\n",
    "g_path = '../graph_construction/combined_graphs_output/synthetic_benchmark_large/bipartite/bipartite.graph'\n",
    "G = pickle.load(open(g_path, \"rb\"))\n",
    "\n",
    "node_to_idx_dict_path='../network_analysis/output/synthetic_example_large/node_to_idx_dict.json'\n",
    "with open(node_to_idx_dict_path, 'r') as f:\n",
    "    node_to_idx_dict=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the `hom_to_cluster_ids_dict`\n",
    "hom_to_cluster_ids_dict={}\n",
    "for hom in os.listdir(input_dir):\n",
    "    cur_cluster_ids_dict=get_cluster_assignment(files_path=input_dir+hom)\n",
    "    hom_to_cluster_ids_dict[hom]=cur_cluster_ids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 3140.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store the chatGPT clusters in readable form\n",
    "output_dir='readable_clusters/synthetic_benchmark_large/gpt/'\n",
    "for hom in tqdm(hom_to_cluster_ids_dict):\n",
    "    cluster_id_to_columns_dict={}\n",
    "    for idx, val in node_to_idx_dict[hom].items():\n",
    "        if val not in hom_to_cluster_ids_dict[hom]:\n",
    "            cur_cluster_id=0\n",
    "        else:\n",
    "            cur_cluster_id=hom_to_cluster_ids_dict[hom][val]\n",
    "        column_name=G.nodes[val]['column_name']\n",
    "\n",
    "        if cur_cluster_id not in cluster_id_to_columns_dict:\n",
    "            cluster_id_to_columns_dict[cur_cluster_id]=[column_name]\n",
    "        else:\n",
    "            cluster_id_to_columns_dict[cur_cluster_id].append(column_name)\n",
    "\n",
    "    with open(output_dir+hom+'.json', 'w') as f:\n",
    "        json.dump(cluster_id_to_columns_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 3314.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store the KDE computed clusters in readable form\n",
    "output_dir='readable_clusters/synthetic_benchmark_large/kde/'\n",
    "with open('../network_analysis/output/synthetic_example_large/kde_labels.pickle', 'rb') as f:\n",
    "    kde_labels=pickle.load(f)\n",
    "\n",
    "for hom in tqdm(kde_labels):\n",
    "    cluster_id_to_columns_dict={}\n",
    "    for i in range(len(kde_labels[hom])):\n",
    "        column_name=G.nodes[node_to_idx_dict[hom][str(i)]]['column_name']\n",
    "        cur_cluster_id=int(kde_labels[hom][i])\n",
    "        \n",
    "        if cur_cluster_id not in cluster_id_to_columns_dict:\n",
    "            cluster_id_to_columns_dict[cur_cluster_id]= [column_name]\n",
    "        else:\n",
    "            cluster_id_to_columns_dict[cur_cluster_id].append(column_name)\n",
    "        \n",
    "    with open(output_dir+hom+'.json', 'w') as f:\n",
    "        json.dump(cluster_id_to_columns_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 43/180 [00:00<00:00, 421.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 433.44it/s]\n"
     ]
    }
   ],
   "source": [
    "homograph_eval_dict={}\n",
    "# Perform Evaluation over each homograph\n",
    "for hom in tqdm(hom_to_cluster_ids_dict):\n",
    "    # Build the `labels_pred` in the order it appears in `node_to_idx_dict`\n",
    "    labels_pred=[]\n",
    "    for idx, val in node_to_idx_dict[hom].items():\n",
    "        if val not in hom_to_cluster_ids_dict[hom]:\n",
    "            labels_pred.append(0)\n",
    "        else:\n",
    "            labels_pred.append(hom_to_cluster_ids_dict[hom][val])\n",
    "    adj_rand_idx_gpt=get_clustering_evaluation_score(labels_pred=labels_pred, idx_to_nodes=node_to_idx_dict[hom], G=G, measure='adj_rand_index')\n",
    "    num_meanings_gpt=len(set(labels_pred))\n",
    "    homograph_eval_dict[hom]={'adj_rand_index': adj_rand_idx_gpt, 'num_meanings': num_meanings_gpt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>node_type</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>is_homograph</th>\n",
       "      <th>dense_rank</th>\n",
       "      <th>num_meanings_groundtruth</th>\n",
       "      <th>num_meanings_greatest</th>\n",
       "      <th>num_meanings_kde</th>\n",
       "      <th>epsilon_greatest</th>\n",
       "      <th>epsilon_kde</th>\n",
       "      <th>...</th>\n",
       "      <th>is_greatest_num_meanings_correct</th>\n",
       "      <th>greatest_adj_rand_index</th>\n",
       "      <th>greatest_adj_mutual_info</th>\n",
       "      <th>greatest_norm_mutual_info</th>\n",
       "      <th>kde_adj_rand_index</th>\n",
       "      <th>kde_adj_mutual_info</th>\n",
       "      <th>kde_norm_mutual_info</th>\n",
       "      <th>num_meanings_gpt</th>\n",
       "      <th>is_gpt_num_meanings_correct</th>\n",
       "      <th>gpt_adj_rand_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7590</th>\n",
       "      <td>Palm</td>\n",
       "      <td>cell</td>\n",
       "      <td>1.445700e-02</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.970414</td>\n",
       "      <td>0.974673</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25825</th>\n",
       "      <td>Clementine</td>\n",
       "      <td>cell</td>\n",
       "      <td>8.975958e-03</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.812009</td>\n",
       "      <td>0.830499</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>cell</td>\n",
       "      <td>8.132084e-03</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>Jaguar</td>\n",
       "      <td>cell</td>\n",
       "      <td>8.049376e-03</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.692718</td>\n",
       "      <td>0.825997</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.129032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7416</th>\n",
       "      <td>Timothy</td>\n",
       "      <td>cell</td>\n",
       "      <td>6.921700e-03</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>MD</td>\n",
       "      <td>cell</td>\n",
       "      <td>2.103537e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>275.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.469551</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>SD</td>\n",
       "      <td>cell</td>\n",
       "      <td>2.059766e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.913907</td>\n",
       "      <td>0.459670</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>ME</td>\n",
       "      <td>cell</td>\n",
       "      <td>1.214843e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>314.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.907285</td>\n",
       "      <td>0.438742</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>NC</td>\n",
       "      <td>cell</td>\n",
       "      <td>7.969017e-08</td>\n",
       "      <td>True</td>\n",
       "      <td>329.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.912752</td>\n",
       "      <td>0.358959</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>LA</td>\n",
       "      <td>cell</td>\n",
       "      <td>7.900777e-08</td>\n",
       "      <td>True</td>\n",
       "      <td>330.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.359842</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             node node_type  betweenness_centrality is_homograph  dense_rank  \\\n",
       "7590         Palm      cell            1.445700e-02         True         1.0   \n",
       "25825  Clementine      cell            8.975958e-03         True         2.0   \n",
       "7130     Magnolia      cell            8.132084e-03         True         3.0   \n",
       "3077       Jaguar      cell            8.049376e-03         True         4.0   \n",
       "7416      Timothy      cell            6.921700e-03         True         5.0   \n",
       "...           ...       ...                     ...          ...         ...   \n",
       "2000           MD      cell            2.103537e-07         True       275.0   \n",
       "1828           SD      cell            2.059766e-07         True       276.0   \n",
       "2004           ME      cell            1.214843e-07         True       314.0   \n",
       "1950           NC      cell            7.969017e-08         True       329.0   \n",
       "1945           LA      cell            7.900777e-08         True       330.0   \n",
       "\n",
       "       num_meanings_groundtruth  num_meanings_greatest  num_meanings_kde  \\\n",
       "7590                          2                    2.0               2.0   \n",
       "25825                         2                    2.0               2.0   \n",
       "7130                          2                    2.0               2.0   \n",
       "3077                          2                    2.0               2.0   \n",
       "7416                          2                    2.0               2.0   \n",
       "...                         ...                    ...               ...   \n",
       "2000                          2                    1.0               2.0   \n",
       "1828                          2                    1.0               2.0   \n",
       "2004                          2                    1.0               2.0   \n",
       "1950                          2                    1.0               2.0   \n",
       "1945                          2                    1.0               2.0   \n",
       "\n",
       "       epsilon_greatest  epsilon_kde  ... is_greatest_num_meanings_correct  \\\n",
       "7590           0.970414     0.974673  ...                             True   \n",
       "25825          0.812009     0.830499  ...                             True   \n",
       "7130                NaN          NaN  ...                             True   \n",
       "3077           0.692718     0.825997  ...                             True   \n",
       "7416                NaN          NaN  ...                             True   \n",
       "...                 ...          ...  ...                              ...   \n",
       "2000           0.908497     0.469551  ...                            False   \n",
       "1828           0.913907     0.459670  ...                            False   \n",
       "2004           0.907285     0.438742  ...                            False   \n",
       "1950           0.912752     0.358959  ...                            False   \n",
       "1945           0.921569     0.359842  ...                            False   \n",
       "\n",
       "       greatest_adj_rand_index  greatest_adj_mutual_info  \\\n",
       "7590                       1.0                       1.0   \n",
       "25825                      1.0                       1.0   \n",
       "7130                       1.0                       0.0   \n",
       "3077                       1.0                       1.0   \n",
       "7416                       1.0                       0.0   \n",
       "...                        ...                       ...   \n",
       "2000                       0.0                       0.0   \n",
       "1828                       0.0                       0.0   \n",
       "2004                       0.0                       0.0   \n",
       "1950                       0.0                       0.0   \n",
       "1945                       0.0                       0.0   \n",
       "\n",
       "       greatest_norm_mutual_info  kde_adj_rand_index  kde_adj_mutual_info  \\\n",
       "7590                         1.0                 1.0                  1.0   \n",
       "25825                        1.0                 1.0                  1.0   \n",
       "7130                         1.0                 1.0                  0.0   \n",
       "3077                         1.0                 1.0                  1.0   \n",
       "7416                         1.0                 1.0                  0.0   \n",
       "...                          ...                 ...                  ...   \n",
       "2000                         0.0                 1.0                  1.0   \n",
       "1828                         0.0                 1.0                  1.0   \n",
       "2004                         0.0                 1.0                  1.0   \n",
       "1950                         0.0                 1.0                  1.0   \n",
       "1945                         0.0                 1.0                  1.0   \n",
       "\n",
       "       kde_norm_mutual_info  num_meanings_gpt  is_gpt_num_meanings_correct  \\\n",
       "7590                    1.0                 3                        False   \n",
       "25825                   1.0                 1                        False   \n",
       "7130                    1.0                 1                        False   \n",
       "3077                    1.0                 5                        False   \n",
       "7416                    1.0                 2                         True   \n",
       "...                     ...               ...                          ...   \n",
       "2000                    1.0                 6                        False   \n",
       "1828                    1.0                 6                        False   \n",
       "2004                    1.0                 3                        False   \n",
       "1950                    1.0                 3                        False   \n",
       "1945                    1.0                 3                        False   \n",
       "\n",
       "       gpt_adj_rand_index  \n",
       "7590             0.333333  \n",
       "25825            0.000000  \n",
       "7130             0.000000  \n",
       "3077            -0.129032  \n",
       "7416             1.000000  \n",
       "...                   ...  \n",
       "2000             0.000000  \n",
       "1828             0.000000  \n",
       "2004             0.333333  \n",
       "1950             0.000000  \n",
       "1945             0.000000  \n",
       "\n",
       "[180 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the eval_df to contain evaluation values for gpt extraction \n",
    "eval_df=pd.read_pickle('../network_analysis/output/synthetic_example_large/homographs_clustering_eval_df.pickle')\n",
    "num_meanings_gpt_list=[]\n",
    "is_gpt_num_meanings_correct_list=[]\n",
    "gpt_adj_rand_index=[]\n",
    "for idx, row in eval_df.iterrows():\n",
    "    num_meanings=homograph_eval_dict[row['node']]['num_meanings']\n",
    "    is_gpt_num_meanings_correct=row['num_meanings_groundtruth']==num_meanings\n",
    "    adj_rand_idx=homograph_eval_dict[row['node']]['adj_rand_index']\n",
    "    num_meanings_gpt_list.append(num_meanings)\n",
    "    is_gpt_num_meanings_correct_list.append(is_gpt_num_meanings_correct)\n",
    "    gpt_adj_rand_index.append(adj_rand_idx)\n",
    "\n",
    "eval_df['num_meanings_gpt']=num_meanings_gpt_list\n",
    "eval_df['is_gpt_num_meanings_correct']=is_gpt_num_meanings_correct_list\n",
    "eval_df['gpt_adj_rand_index']=gpt_adj_rand_index\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of meanings AVG precision using KDE is: 0.9722222222222222 and using chatGPT is: 0.45555555555555555\n"
     ]
    }
   ],
   "source": [
    "kde_precision = eval_df['is_kde_num_meanings_correct'].value_counts()[True] / len(eval_df.index)\n",
    "gpt_precision = eval_df['is_gpt_num_meanings_correct'].value_counts()[True] / len(eval_df.index)\n",
    "print(\"Number of meanings AVG precision using KDE is:\", kde_precision, \"and using chatGPT is:\", gpt_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG adj rand index using KDE is: 0.9911270945693412 and using chatGPT is: 0.43870604794489404\n"
     ]
    }
   ],
   "source": [
    "print(\"AVG adj rand index using KDE is:\", eval_df['kde_adj_rand_index'].mean(), \"and using chatGPT is:\", eval_df['gpt_adj_rand_index'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Pair Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_prediction(pair, pred_labels, node_to_idx_dict):\n",
    "    '''\n",
    "    Given a pair (column_filename, column_filename) return two booleans\n",
    "    \n",
    "    1) the gt (i.e., part of the same or not cluster)\n",
    "    2) whether DomainNet predicted that they are part of the same or different cluster\n",
    "    '''\n",
    "    file1=pair.split('___')[0] + '.csv'\n",
    "    file2=pair.split('___')[1].split('.')[0] + '.csv'\n",
    "\n",
    "    filename_to_idx={}\n",
    "    for idx, val in node_to_idx_dict.items():\n",
    "        filename_to_idx[val]=int(idx)\n",
    "    \n",
    "    # GT relation between two files\n",
    "    col1, col2 = G.nodes[file1]['column_name'], G.nodes[file2]['column_name']\n",
    "    if col1 == col2:\n",
    "        gt=False\n",
    "    else:\n",
    "        gt=True\n",
    "\n",
    "    # DomainNet Prediction\n",
    "    file1_pred=pred_labels[filename_to_idx[file1]]\n",
    "    file2_pred=pred_labels[filename_to_idx[file2]]\n",
    "    if file1_pred == file2_pred:\n",
    "        pred=False\n",
    "    else:\n",
    "        pred=True\n",
    "\n",
    "    return gt, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 70/180 [00:00<00:00, 601.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 769.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homograph</th>\n",
       "      <th>pair</th>\n",
       "      <th>is_gpt_pred_correct</th>\n",
       "      <th>is_domain_net_pred_correct</th>\n",
       "      <th>gpt_pred</th>\n",
       "      <th>domain_net_pred</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Irving</td>\n",
       "      <td>first_name_personal_first_name_last_name_ssn_s...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Irving</td>\n",
       "      <td>city_location_city_country_3___city_location_c...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Irving</td>\n",
       "      <td>first_name_personal_first_name_last_name_ssn_s...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thorpe</td>\n",
       "      <td>first_name_personal_first_name_last_name_ssn_s...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thorpe</td>\n",
       "      <td>first_name_personal_first_name_last_name_ssn_s...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>Clementine</td>\n",
       "      <td>grocery_product_grocery_country_2___grocery_pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>Luther</td>\n",
       "      <td>last_name_personal_first_name_last_name_ssn_st...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>Denver</td>\n",
       "      <td>first_name_personal_first_name_last_name_ssn_s...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>Denver</td>\n",
       "      <td>first_name_personal_first_name_last_name_ssn_s...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>Anastasia</td>\n",
       "      <td>movie_title_product_movie_title_movie_genre_co...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1581 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       homograph                                               pair  \\\n",
       "0         Irving  first_name_personal_first_name_last_name_ssn_s...   \n",
       "1         Irving  city_location_city_country_3___city_location_c...   \n",
       "2         Irving  first_name_personal_first_name_last_name_ssn_s...   \n",
       "3         Thorpe  first_name_personal_first_name_last_name_ssn_s...   \n",
       "4         Thorpe  first_name_personal_first_name_last_name_ssn_s...   \n",
       "...          ...                                                ...   \n",
       "1576  Clementine  grocery_product_grocery_country_2___grocery_pr...   \n",
       "1577      Luther  last_name_personal_first_name_last_name_ssn_st...   \n",
       "1578      Denver  first_name_personal_first_name_last_name_ssn_s...   \n",
       "1579      Denver  first_name_personal_first_name_last_name_ssn_s...   \n",
       "1580   Anastasia  movie_title_product_movie_title_movie_genre_co...   \n",
       "\n",
       "      is_gpt_pred_correct  is_domain_net_pred_correct  gpt_pred  \\\n",
       "0                    True                        True      True   \n",
       "1                   False                        True      True   \n",
       "2                    True                        True      True   \n",
       "3                   False                        True      True   \n",
       "4                    True                        True      True   \n",
       "...                   ...                         ...       ...   \n",
       "1576                False                        True      True   \n",
       "1577                 True                        True      True   \n",
       "1578                 True                        True      True   \n",
       "1579                 True                        True      True   \n",
       "1580                 True                        True      True   \n",
       "\n",
       "      domain_net_pred     gt  \n",
       "0                True   True  \n",
       "1               False  False  \n",
       "2                True   True  \n",
       "3               False  False  \n",
       "4                True   True  \n",
       "...               ...    ...  \n",
       "1576            False  False  \n",
       "1577             True   True  \n",
       "1578             True   True  \n",
       "1579             True   True  \n",
       "1580             True   True  \n",
       "\n",
       "[1581 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a per-pair evaluation between GPT and DomainNet\n",
    "gpt_path='gpt_output/synthetic_benchmark_large/'\n",
    "node_to_idx_dict_path='../network_analysis/output/synthetic_example_large/node_to_idx_dict.json'\n",
    "with open('../network_analysis/output/synthetic_example_large/kde_labels.pickle', 'rb') as f:\n",
    "    kde_labels=pickle.load(f)\n",
    "with open(node_to_idx_dict_path, 'r') as f:\n",
    "    node_to_idx_dict=json.load(f)\n",
    "g_path = '../graph_construction/combined_graphs_output/synthetic_benchmark_large/bipartite/bipartite.graph'\n",
    "G = pickle.load(open(g_path, \"rb\"))\n",
    "\n",
    "pair_eval_dict={\"homograph\": [], \"pair\": [], \"is_gpt_pred_correct\": [], \"is_domain_net_pred_correct\": [], \"gpt_pred\": [], \"domain_net_pred\": [], \"gt\": []}\n",
    "for hom in tqdm(os.listdir(gpt_path)):\n",
    "    for pair in os.listdir(gpt_path+hom+'/'):\n",
    "        with open(gpt_path+hom+'/'+pair, 'r') as f:\n",
    "            gpt_pred=json.load(f)['is_homograph']\n",
    "        if gpt_pred == 'True':\n",
    "            gpt_pred=True\n",
    "        else:\n",
    "            gpt_pred=False\n",
    "        \n",
    "        # Get GT and DomainNet Predictions\n",
    "        gt, domain_net_pred = get_pair_prediction(pair=pair, pred_labels=kde_labels[hom], node_to_idx_dict=node_to_idx_dict[hom])\n",
    "        \n",
    "        if gpt_pred==gt:\n",
    "            is_gpt_pred_correct=True\n",
    "        else:\n",
    "            is_gpt_pred_correct=False\n",
    "        if domain_net_pred==gt:\n",
    "            is_domain_net_pred_correct=True\n",
    "        else:\n",
    "            is_domain_net_pred_correct=False\n",
    "        \n",
    "        pair_eval_dict['homograph'].append(hom)\n",
    "        pair_eval_dict['pair'].append(pair)\n",
    "        pair_eval_dict['is_gpt_pred_correct'].append(is_gpt_pred_correct)\n",
    "        pair_eval_dict['is_domain_net_pred_correct'].append(is_domain_net_pred_correct)\n",
    "        pair_eval_dict['gpt_pred'].append(gpt_pred)\n",
    "        pair_eval_dict['domain_net_pred'].append(domain_net_pred)\n",
    "        pair_eval_dict['gt'].append(gt)\n",
    "\n",
    "pair_eval_df = pd.DataFrame.from_dict(pair_eval_dict)\n",
    "pair_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG GPT pair evaluation accuracy: 0.4845034788108792 AVG DomainNet pair evaluation accuracy: 0.9955724225173941\n"
     ]
    }
   ],
   "source": [
    "gpt_pred_accuracy=pair_eval_df['is_gpt_pred_correct'].value_counts()[True] / len(pair_eval_df)\n",
    "domain_net_accuracy=pair_eval_df['is_domain_net_pred_correct'].value_counts()[True] / len(pair_eval_df)\n",
    "print(\"AVG GPT pair evaluation accuracy:\", gpt_pred_accuracy, \"AVG DomainNet pair evaluation accuracy:\", domain_net_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'icl_ind_0': 'Table:\\n18772|16032|\"Birch Hills\"|0|0|0|25|1|PNR|SK|101|\"Tisdale - CN\"\\nTable:\\n\"No summary - Aucun sommaire\"|\"Electrical and electronic machinery and equipment (including computer hardware)\"|\"Computing and Information Sciences - A\"\\nUnionable: no',\n",
       " 'icl_ind_1': 'Table:\\n4693|12658|\"Hwy  665\"|50.0175|-92.8846|\"ON -Ministry of Transportation\"|Passive|0|19|CN|50|50|2|1|PNR|MB|48.46|Redditt\\nTable:\\n\"No summary - Aucun sommaire\"|\"Electrical and electronic machinery and equipment (including computer hardware)\"|\"Computing and Information Sciences - A\"\\nUnionable: no',\n",
       " 'icl_ind_2': 'Table:\\n18772|16032|\"Birch Hills\"|0|0|0|25|1|PNR|SK|101|\"Tisdale - CN\"\\nTable:\\n\"No summary - Aucun sommaire\"|\"Computer communications\"|\"Computing and Information Sciences - A\"\\nUnionable: no',\n",
       " 'icl_ind_3': 'Table:\\n12658|\"Hwy  665\"|-92.8846|\"ON -Ministry of Transportation\"|Passive|0|0|CN|50|55|2|1|MB|48.46|Redditt|\\nTable:\\n\"No summary - Aucun sommaire\"|\"Electrical and electronic machinery and equipment (including computer hardware)\"|\"Computing and Information Sciences - A\"\\nUnionable: no',\n",
       " 'icl_ind_4': 'Table:\\nB6100|America|HT|Haiti|D6211|\"KSD Social Development Partnerships\"|Health|\"Civil Society\"|\"International Child Care (Canada) Inc.\"|\"Canadian Non-Profit Making\"|S064143002|S6212\\nTable:\\n\"No summary - Aucun sommaire\"|\"Electrical and electronic machinery and equipment (including computer hardware)\"|\"Computing and Information Sciences - A\"\\nUnionable: no'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tus_icl_examples.pickle', 'rb') as f:\n",
    "    examples=pickle.load(f)\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homograph</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Cuba</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Jamaica</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ID</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>AZ</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PA</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>CA</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MA</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>TN</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>AL</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MN</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>CO</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AR</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>DE</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>GT</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>MG</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MT</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>TL</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>NE</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>IL</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>CL</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ES</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>XK</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Montana</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>California</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MD</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Jaguar</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>SD</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Washington</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Pueblo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Tacoma</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Tucson</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>IS</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Cherokee</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>SC</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Aurora</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Seminole</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GA</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TT</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ford</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Jimmy</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Viva</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Palm</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Comanche</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      homograph  count\n",
       "69         Cuba    105\n",
       "169     Jamaica     91\n",
       "59      Georgia     78\n",
       "98           ID     45\n",
       "138          AZ     45\n",
       "29           PA     45\n",
       "171          CA     45\n",
       "22           MA     45\n",
       "166          TN     45\n",
       "165          AL     45\n",
       "36           MN     45\n",
       "75           CO     45\n",
       "11           AR     36\n",
       "170          DE     36\n",
       "118          GT     28\n",
       "134          MG     28\n",
       "16           MT     28\n",
       "28     Colorado     28\n",
       "103          TL     21\n",
       "131          NE     21\n",
       "163          IL     21\n",
       "73      Lincoln     21\n",
       "173    Delaware     21\n",
       "61           CL     21\n",
       "172    Virginia     15\n",
       "30           ES     15\n",
       "160          XK     15\n",
       "70      Montana     15\n",
       "100  California     15\n",
       "122    Nebraska     15\n",
       "21           MD     15\n",
       "110      Jaguar     15\n",
       "89           SD     15\n",
       "88   Washington     15\n",
       "56       Pueblo     10\n",
       "85       Tacoma     10\n",
       "91       Kansas     10\n",
       "62       Tucson     10\n",
       "57           IS     10\n",
       "136    Cherokee     10\n",
       "139          SC     10\n",
       "42       Aurora     10\n",
       "120    Seminole     10\n",
       "27           GA     10\n",
       "26           TT     10\n",
       "23         Ford     10\n",
       "43        Jimmy      6\n",
       "168        Viva      6\n",
       "81         Palm      6\n",
       "147    Comanche      6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pairs_per_homograph_dict={\"homograph\": [], \"count\": []}\n",
    "dir=\"gpt_queries/synthetic_benchmark_large/gpt_formed_query/\"\n",
    "for hom in os.listdir(dir):\n",
    "    num_pairs_per_homograph_dict['homograph'].append(hom)\n",
    "    num_pairs_per_homograph_dict['count'].append(len(os.listdir(dir+hom)))\n",
    "num_pairs_per_homograph=pd.DataFrame.from_dict(num_pairs_per_homograph_dict)\n",
    "num_pairs_per_homograph.sort_values(by='count', ascending=False).head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
